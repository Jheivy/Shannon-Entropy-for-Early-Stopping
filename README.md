# âœ¨ Shannon Entropy for Early Stopping: A New Criterion for Neural Networks

### Abstract
Identifying the optimal point for early stopping in neural network training is a challenge. In this paper, we propose a novel analytical approach based on statistical physics. We study the Shannon entropy of the weights of the second hidden layer ($W_2$) of a multilayer perceptron (MLP) trained on the MNIST dataset. The results show that the entropy of the weights of this layer reaches a minimum of uncertainty just when the network achieves maximum generalisation capacity. This minimum entropy establishes a balance between minimising validation loss and the internal stability of the weights. We conclude that the entropy of the weights in the output layer is a robust criterion for early detection, providing a deep and theoretically grounded insight into the internal state of the model.


### Results


The analysis of Shannon entropy applied to the weights of an artificial neural network has provided a novel perspective and a valuable complement to traditional performance metrics based on validation loss monitoring. In this work, we have demonstrated that a different analytical approach, inspired by statistical physics, can provide deeper insight into the internal state of the neural network.  Shannon entropy analysis of the weights of the output layer $W_2$ has revealed a minimum in entropy.

The most significant finding of the study is the correlation between the minimum entropy of the weights and the point of maximum stability of the model. While the traditional early stopping criterion would have suggested continuing training until iteration 15,800 to reach the minimum validation loss, the results indicate that the optimal and most stable configuration of the weights was achieved much earlier, at iteration 7,800. The gain in validation loss between these two points was marginal in exchange for an increase in the uncertainty of the weight matrix of $W_2$. This phenomenon suggests a slight overfitting that traditional metrics are unable to detect.

Unlike previous works that use entropy as a loss function, the approach taken to entropy in this work is that of a study metric, without modifying the optimisation process. The results observed for $W_1$ and $W_2$ show that this metric is more sensitive in the output layers.

### Conclusions
The key findings are as follows:


  - The entropy of the output layer weights ($W_2$) exhibits predictable behaviour throughout training, reaching a minimum of uncertainty just when the model achieves its maximum stability and generalisation.
  - This minimum entropy occurs significantly before the point of minimum validation loss. Continuing training until the minimum validation loss resulted in marginal performance improvement at the expense of the model's internal stability, suggesting the presence of slight overfitting that traditional metrics are unable to detect.
  - Using weight entropy as an early stopping criterion offers a more efficient and robust solution, as it allows training to be stopped at an optimal point of balance between performance and stability, preventing overfitting in a more conservative manner.

The clear divergence between the entropy of the weights of the input layer ($W_1$) and the output layer ($W_2$) highlights the importance of monitoring the most specialised layers to obtain reliable indicators of the model's status. This approach, inspired by statistical physics, validates a new perspective for understanding the internal dynamics of machine learning.

``` Python
  git clone https://github.com/Jheivy/Shannon-Entropy-for-Early-Stopping.git
